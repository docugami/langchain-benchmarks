{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10b060d-6363-4669-8866-7c8ef1103a38",
   "metadata": {},
   "source": [
    "# Semi-structured eval: Unstructured + Multi-vector retriever\n",
    "\n",
    "We will test retrieval of table information from the `Semi-structured Reports` dataset using various methods.\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae46dade-a12b-4ee4-abfa-c5b071a7eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U langchain langsmith langchain_benchmarks\n",
    "%pip install --quiet chromadb openai \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daca152-b63b-4403-9b46-66ff9ab2a275",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafb546c-32a1-46e9-b0aa-9b4b2c439324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_benchmarks import registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]\n",
    "\n",
    "### TODO: Replace when dataset is updated\n",
    "#dir = \"/Users/rlm/Desktop/Eval_Sets/semi_structured_reports/\"\n",
    "dir = \"/mnt/c/Users/taqi_/OneDrive/Desktop/semi_structured_reports/\"\n",
    "files = [f for f in os.listdir(dir) if f.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce580b-cdbf-4e09-8273-35a9e0b62307",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "Use table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63985459-b295-422a-a5ca-1fba86049964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 tables\n",
      "There are 21 text elements\n",
      "There are 1 tables\n",
      "There are 5 text elements\n",
      "There are 9 tables\n",
      "There are 11 text elements\n",
      "There are 13 tables\n",
      "There are 15 text elements\n",
      "There are 12 tables\n",
      "There are 14 text elements\n",
      "There are 2 tables\n",
      "There are 5 text elements\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def hash_table(table):\n",
    "    \"\"\"\n",
    "    Generate a hash for a table based on its content.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(str(table).encode()).hexdigest()\n",
    "\n",
    "\n",
    "def generate_doc_summary(file):\n",
    "    \"\"\"\n",
    "    Create a doc summary\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing a document. \\\n",
    "    Write a concisde 1-2 sentence summary of the document: {document} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
    "    summarize_chain = {\"document\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Load doc\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "    texts = [t.page_content for t in pdf_pages]\n",
    "    text_string = \" \".join(texts)\n",
    "    summary = summarize_chain.invoke({\"document\": text_string})\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    \"\"\"\n",
    "    Get tables, texts from pdf\n",
    "    \"\"\"\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "texts = []\n",
    "tables = []\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "for fi in files:\n",
    "    # Generate document summary\n",
    "    doc_summary = generate_doc_summary(dir + fi)\n",
    "\n",
    "    # Get texts, tables\n",
    "    doc_texts, doc_tables = parse_pdf_unstructured(dir + fi)\n",
    "\n",
    "    # Enforce a specific token size for texts to summarize larger chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=4000, chunk_overlap=0\n",
    "    )\n",
    "    joined_doc_texts = \" \".join(doc_texts)\n",
    "    doc_texts = text_splitter.split_text(joined_doc_texts)\n",
    "\n",
    "    # Get text, table summaries\n",
    "    doc_text_summaries, doc_table_summaries = generate_text_summaries(\n",
    "        doc_texts, doc_tables, summarize_texts=True\n",
    "    )\n",
    "\n",
    "    # Add doc summary to table summary to preserve context\n",
    "    doc_table_summaries = [\n",
    "        doc_summary + \"\\n here is a table summary in this doc:\" + t\n",
    "        for t in doc_table_summaries\n",
    "    ]\n",
    "\n",
    "    # Add to lists\n",
    "    texts.extend(doc_texts)\n",
    "    tables.extend(doc_tables)\n",
    "    text_summaries.extend(doc_text_summaries)\n",
    "    table_summaries.extend(doc_table_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8e2db-98da-4867-9c1a-c1d59f2b3d41",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c898d14-1ca6-477f-93bd-3ec942f02748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries=None,\n",
    "    texts=None,\n",
    "    table_summaries=None,\n",
    "    tables=None,\n",
    "    image_summaries=None,\n",
    "    images=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3bee7d-8cf4-4896-8111-323f4ddcbfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"unstructured-tables\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    table_summaries=table_summaries,\n",
    "    tables=tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb11d59-677a-47a8-8225-0be5c85325db",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705a9e67-09b1-47a6-8280-db4a77052a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain = rag_chain(retriever_unstructured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35eefd-eef1-4c51-a5d8-03f45b2b21d9",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec0906a-1eaf-415e-9fcd-06e2efb9ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'unstructured_40142856-2364-46b2-bee6-dd6175da958d' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/projects/p/f900fab4-2ef0-420d-97d2-31172e9ce97e?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-Structured-Eval-v5 at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/2759f13d-c0c0-4d60-a8cf-0ce204750642\n",
      "[------------------------------------------------->] 25/25\n",
      " Eval quantiles:\n",
      "                                          inputs.question  \\\n",
      "count                                                  25   \n",
      "unique                                                 25   \n",
      "top     What is Datadog's Non-GAAP gross margin for th...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "        feedback.COT Contextual Accuracy error  execution_time  \n",
      "count                          25.000000     0       25.000000  \n",
      "unique                               NaN     0             NaN  \n",
      "top                                  NaN   NaN             NaN  \n",
      "freq                                 NaN   NaN             NaN  \n",
      "mean                            0.480000   NaN        4.324700  \n",
      "std                             0.509902   NaN        1.230167  \n",
      "min                             0.000000   NaN        2.840166  \n",
      "25%                             0.000000   NaN        3.612251  \n",
      "50%                             0.000000   NaN        3.920019  \n",
      "75%                             1.000000   NaN        4.534860  \n",
      "max                             1.000000   NaN        7.452328  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>The text does not provide information on the p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.202974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.212825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.637487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.431906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.952879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.655802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.212505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                  20   \n",
       "unique                                                 20   \n",
       "top     The text does not provide information on the p...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy error  execution_time  \n",
       "count                          20.000000     0       20.000000  \n",
       "unique                               NaN     0             NaN  \n",
       "top                                  NaN   NaN             NaN  \n",
       "freq                                 NaN   NaN             NaN  \n",
       "mean                            0.400000   NaN        5.202974  \n",
       "std                             0.502625   NaN        1.212825  \n",
       "min                             0.000000   NaN        3.637487  \n",
       "25%                             0.000000   NaN        4.431906  \n",
       "50%                             0.000000   NaN        4.952879  \n",
       "75%                             1.000000   NaN        5.655802  \n",
       "max                             1.000000   NaN        9.212505  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "from langsmith.client import Client\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        ### TODO: Replace with public dataset\n",
    "        dataset_name=\"Semi-Structured-Eval-v5\",\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"unstructured\": chain,\n",
    "}\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name + \"_\" + run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
