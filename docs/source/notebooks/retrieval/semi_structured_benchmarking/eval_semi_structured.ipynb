{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684707be-0b26-417f-92d5-5e11fc49a923",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Get Public Dataset\n",
    "\n",
    "Get the LangSmith public dataset for semi-structured data, `Semi-structured Reports`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cae381c-009d-40a7-84e0-c291b7f502c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tiktoken openai chromadb --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c213899-73dd-4043-b331-c15d818e59a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>Semi-structured Reports                                                                                                                                    </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td></tr>\n",
       "<tr><td>Description           </td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>basic, parent-doc, hyde                                                                                                                                    </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>get_docs              </td><td><function load_docs at 0x118b361f0>                                                                                                                        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x118b36280>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x118b36310>, 'hyde': <function _chroma_hyde_retriever_factory at 0x118b363a0>}, architecture_factories={}, get_docs=<function load_docs at 0x118b361f0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24496c9-966e-4a99-92af-7972b317b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fc5fc-e8c9-4086-8aa8-37a2eb401cc1",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "PDF loader (naive to tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdff4e1-aba7-4529-ac2f-9d9db09a0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pypdf --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23948b39-e22d-4233-bc4c-1173a0bded0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 text elements\n",
      "There are 3 text elements\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def load_and_split(file):\n",
    "    \"\"\"\n",
    "    Load and split PDF files\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    # Split\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2000, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Get chunks\n",
    "    docs = text_splitter.split_documents(pdf_pages)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "baseline_texts = []\n",
    "for fi in files:\n",
    "    baseline_texts += load_and_split(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb645-763a-44b6-883e-de55fe8df811",
   "metadata": {},
   "source": [
    "### Unstructured\n",
    "\n",
    "Table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9c71ed-985d-44ce-9770-e100b78ba3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"unstructured[all-docs]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d2fbe3-c834-4119-9d53-bb3881b3d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 tables\n",
      "There are 11 text elements\n",
      "There are 1 tables\n",
      "There are 5 text elements\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# Run\n",
    "unstructured_texts = []\n",
    "unstructured_tables = []\n",
    "for fi in files:\n",
    "    texts, tables = parse_pdf_unstructured(fi)\n",
    "    unstructured_texts += texts\n",
    "    unstructured_tables += tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1432f-654d-4b0c-9f92-14c7cada9237",
   "metadata": {},
   "source": [
    "### Docugami\n",
    "\n",
    "Table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/docugami_xml_kg_rag.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b57330a-0b43-4135-9901-05002ce09af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install docugami==0.0.7 dgml-utils==0.2.1 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e09cbb-5758-4ff9-bce5-80730ab2313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami import Docugami, upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "# Load\n",
    "DOCSET_NAME = \"Semi-Structured\"\n",
    "dg_client = Docugami()\n",
    "dg_docs = upload_to_named_docset(dg_client, files, DOCSET_NAME)\n",
    "dgml_paths = wait_for_dgml(dg_client, dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799ca7a2-3907-456a-be4c-7de6a43c930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tables\n",
      "There are 14 text elements\n",
      "There are 1 tables\n",
      "There are 15 text elements\n"
     ]
    }
   ],
   "source": [
    "from dgml_utils.segmentation import get_chunks_str\n",
    "\n",
    "\n",
    "def extract_docugami_file(dgml_path):\n",
    "    with open(dgml_path, \"r\") as file:\n",
    "        contents = file.read().encode(\"utf-8\")\n",
    "\n",
    "        # Char to tokens for OpenAI is roughtly a div by 4\n",
    "        # Ref: https://help.openai.com/en/articles/4936856-what-are-token\n",
    "        chunks = get_chunks_str(\n",
    "            contents,\n",
    "            include_xml_tags=True,\n",
    "            max_text_length=1024 * 8,  # ~2k tokens\n",
    "            min_text_length=1024,  # ~0.25k tokens\n",
    "        )\n",
    "\n",
    "    # Tables\n",
    "    tables = [c.text for c in chunks if \"table\" in c.structure.split()]\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "\n",
    "    # Text\n",
    "    texts = [c.text for c in chunks if \"table\" not in c.structure.split()]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "docugami_texts = []\n",
    "docugami_tables = []\n",
    "for fname in files:\n",
    "    # Get xml\n",
    "    dgml_path = dgml_paths[Path(fname).name]\n",
    "    # Extract elements\n",
    "    texts, tables = extract_docugami_file(dgml_path)\n",
    "    docugami_texts += texts\n",
    "    docugami_tables += tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587675d9-c68e-4691-8a6c-796a1c113e6e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80fc789b-1be0-4944-8e8d-a55d2cbe7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "vectorstore_baseline = Chroma.from_texts(\n",
    "    texts=baseline_texts, collection_name=\"baseline\", embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_baseline = vectorstore_baseline.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab563f-b52e-4bf8-8efb-d16f0fe5179b",
   "metadata": {},
   "source": [
    "### Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1580144d-4931-4163-be64-8612d2e4e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries=None,\n",
    "    images=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10967672-aa36-4405-9d85-701e28d79940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unstructured\n",
    "\n",
    "# Get text, table summaries\n",
    "unstructured_text_summaries, unstructured_table_summaries = generate_text_summaries(\n",
    "    unstructured_texts, unstructured_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "unstructured_vectorstore = Chroma(\n",
    "    collection_name=\"unstructured\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    unstructured_vectorstore,\n",
    "    unstructured_text_summaries,\n",
    "    unstructured_texts,\n",
    "    unstructured_table_summaries,\n",
    "    unstructured_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea447967-e16f-4252-999d-08b44e4a2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docugami\n",
    "\n",
    "# Get text, table summaries\n",
    "docugami_text_summaries, docugami_table_summaries = generate_text_summaries(\n",
    "    docugami_texts, docugami_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "docugami_vectorstore = Chroma(\n",
    "    collection_name=\"docugami\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_docugami = create_multi_vector_retriever(\n",
    "    docugami_vectorstore,\n",
    "    docugami_text_summaries,\n",
    "    docugami_texts,\n",
    "    docugami_table_summaries,\n",
    "    docugami_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94ea8f-eb08-492b-90db-96e27ccd5b61",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc530b9-c804-4354-967f-4342431eccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chains\n",
    "chain_baseline = rag_chain(retriever_baseline)\n",
    "chain_unstructured = rag_chain(retriever_unstructured)\n",
    "chain_docugami = rag_chain(retriever_docugami)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebcbab-3207-4012-82b2-9d71d0f0edb1",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "See guide [here](https://github.com/langchain-ai/langchain-benchmarks/blob/main/docs/source/notebooks/retrieval/semi_structured.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d729313-5f39-4a4d-87d3-8cd837ad7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=get_eval_config(),\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"baseline\": chain_baseline,\n",
    "    \"unstructured\": chain_unstructured,\n",
    "    \"docugami\": chain_docugami,\n",
    "    ### TODO: Add images\n",
    "}\n",
    "\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
